{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "Pytorch is a package for python largely used to compute neural network calculations.\n",
    "Its main features include Tensors, Gradient Descend and Modules.\n",
    "Today we will look at what Tensors are and the main functions we can use them for.\n",
    "We will look at Gradient Descent and Modules in future Lessons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "Tensors are a rapresentation of data stored in memory.\n",
    "You can see them as an evolution of numpy arrays. They can have different data type and dimensions.\n",
    "The main feature of Tensors, though, is the computation automatic computation of gradients, useful during BackPropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing a Tensor\n",
    "There are many ways to initialize a tensor. Most of thetime you will initialize it starting from a random distribution or a set of constants.\n",
    "It's also possible to initialize them from already existing data, like Python lists and numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data = [[1, 2],[3, 4]]\n",
    "data_np = np.array(data)\n",
    "x_data = torch.tensor(data)\n",
    "\n",
    "print(data_np, '\\n')\n",
    "print(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor parameters\n",
    "Tensors have many parameters that define how they operate.\n",
    "The most important ones are listed below:\n",
    "- dtype: it's a parameter defining the type of data the tensor contains (integers, float, string...)\n",
    "- shape: it's the shape of the tensor. Like matrixes, some operations between tensors require them to have compatible dimensions\n",
    "- device: this is a technical parameter. It specifies whether a tensor is stored on RAM ('cpu') or on the GPU ('cuda'). Operations between tensors are only possible if the tensors are stored on the same device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'x_data is of type:', x_data.dtype)\n",
    "print(f'x_data has shape:', x_data.shape)\n",
    "print(f'x_data has shape:', x_data.shape[0], x_data.shape[1])\n",
    "print(f'x_data is on:', x_data.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Numpy to Tensor (and viceversa)\n",
    "As stated before, it's possible to create a tensor from already existing data, like numpy arrays. This conversion keeps the data type and shape intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_np = torch.from_numpy(data_np)\n",
    "\n",
    "print(x_np, '\\n')\n",
    "print(x_data)\n",
    "\n",
    "torch_2_np = x_np.numpy()\n",
    "print(data_np, '\\n')\n",
    "print(torch_2_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Tensor from Constants/Random distributions\n",
    "The final way to create a tensor is using one of the built-in constuctors of torch.\n",
    "- ones: creates a tensor of ones\n",
    "- rand: create a tensor filled with numbers taken from a distribution\n",
    "- zeros: creates a tensor filled with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")\n",
    "\n",
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations on Tensors\n",
    "Tensor operate much like a python list or numpy array.\n",
    "- You can select a subset of the tensor\n",
    "- You can select a single value\n",
    "- You can concatenate tensors on a given dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.ones(4, 3)\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First column: {tensor[:, 0]}\")\n",
    "print(f\"Last column: {tensor[..., -1]}\")\n",
    "tensor[:,1] = 0 # set second column to 0\n",
    "print(tensor)\n",
    "\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=1) # concatenate on the column axis\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aritmetic operations with Tensors\n",
    "As expected, all the algebric operations available on matrixes are available on tensors. These are the most popular ones, but remember to look at the documentation if you are looking for a specific function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "y3 = torch.rand_like(y1)\n",
    "torch.matmul(tensor, tensor.T, out=y3)\n",
    "\n",
    "\n",
    "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)\n",
    "\n",
    "z4 = z1 + z3\n",
    "\n",
    "print('z1\\n', z1, '\\nz3\\n', z3, '\\nz4\\n', z4)\n",
    "\n",
    "z5 = z4.to(torch.int32)\n",
    "\n",
    "print(z5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task - Linear Regression Using Torch\n",
    "This exercise is meant to push your python knowledge and force you to use functions that will be crucial in the upcoming lessons.\n",
    "The task in itself is simple: re-implement the exercise of the first lab lesson (Linear regressor with least squares, gradient descent), but this time, using Python classes and torch tensors!\n",
    "\n",
    "You will be provided with a rough structure of how i expect the code to work. The rest is up to you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - LeastSquareRegressor Class\n",
    "Instead of using simple functions for calculating the regression, you have to implement a class LeastSquareRegressor(), following the structure that sklearn uses for its owm regressors.\n",
    "A regressor class should have some parameters and functions that use torch to do the operations seen in lesson 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class LeastSquareRegressor():\n",
    "    \"\"\"\n",
    "    This class should have W has a parameter. This means that when i instantiate a LeastSquareRegressor object, i should be able to access its current W matrix\n",
    "    :attribute W (torch.Tensor): the tensor containing the weigths of this regressor\n",
    "    :attribute b (torch.Tensor): the 1x1 tensor containing the bias\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X : torch.Tensor, Y : torch.Tensor) -> None:\n",
    "        \"\"\"\n",
    "        Given X and Y, calculate W using Least Square solutions\n",
    "        :param X (torch.Tensor): The independent data of shape (N,F)\n",
    "        :param Y (torch.Tensor): The dependent data (labels) of shape (N,1)\n",
    "        :return None\n",
    "        \"\"\"\n",
    "        X = torch.cat([X, torch.ones((X.shape[0],1))], axis=1)\n",
    "\n",
    "        W = torch.linalg.inv(X.transpose(0,1).matmul(X)).matmul(X.transpose(0,1).matmul(Y))\n",
    "\n",
    "        self.W = W[:-1]\n",
    "        self.b = W[-1]\n",
    "    \n",
    "    def regress(self, X) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Given X, the tensor of independent data, calculate Y using the weigths trained\n",
    "        :param X (torch.Tensor): The tensor of independent data of shape (N, F)\n",
    "        :return (torch.Tensor): a tensor of shape (N,1) containing the regressed data\n",
    "        \"\"\"\n",
    "        return X.matmul(self.W) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your progress\n",
    "LSReg = LeastSquareRegressor()\n",
    "print(LSReg.W, LSReg.b)\n",
    "\n",
    "X = torch.rand((5,2))\n",
    "Y = torch.rand((5,1))\n",
    "LSReg.fit(X, Y)\n",
    "print(LSReg.W, LSReg.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Make regression on the boston dataset\n",
    "We have seen in class how to do it, now it's time to do it on your own using torch!\n",
    "\n",
    "TIP: you will need to import the dataset like usual and find a way to convert to a torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "## Import the dataset and create the X and Y tensors, as well as X_np and Y_np, the numpy arrays containing the same data\n",
    "# X and Y should be of type torch.float64\n",
    "\n",
    "def get_data():\n",
    "    X_np, Y_np = load_boston(return_X_y=True)\n",
    "    X = torch.from_numpy(X_np)\n",
    "    Y = torch.from_numpy(Y_np)\n",
    "\n",
    "    return X, Y, X_np, Y_np\n",
    "\n",
    "X, Y, X_np, Y_np = get_data()\n",
    "print(X.shape, Y.shape)\n",
    "print(X_np.shape, Y_np.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from torchmetrics  import R2Score\n",
    "## Fit our regressor to the data!\n",
    "LSReg = LeastSquareRegressor()\n",
    "LSReg.fit(X,Y)\n",
    "print(LSReg.W, LSReg.b, '\\n')\n",
    "y_pred_torch = LSReg.regress(X)\n",
    "R2torch = R2Score()\n",
    "print(f'R2 score of our regressor is {R2torch(y_pred_torch,Y)}\\n\\n')\n",
    "\n",
    "## Compare the results with the sklearn regressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "SKReg = LinearRegression()\n",
    "SKReg.fit(X_np,Y_np)\n",
    "print(SKReg.coef_, SKReg.intercept_, '\\n')\n",
    "y_pred_sk = X_np.dot(SKReg.coef_) + SKReg.intercept_\n",
    "print(f'R2 score of SKlearn regressor is {r2_score(Y_np, y_pred_sk)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Task - Gradient Descent Regressor with pytorch\n",
    "Following what you just did with the LeastSquare regressor, try and implement the Gradient Descent one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDRegressor():\n",
    "    \"\"\"\n",
    "    This class should have W has a parameter. This means that when i instantiate a LeastSquareRegressor object, i should be able to access its current W matrix\n",
    "    :attribute W (torch.Tensor): the tensor containing the weigths of this regressor\n",
    "    :attribute b (torch.Tensor): the 1x1 tensor containing the bias\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        \n",
    "    def fit(self, X : torch.tensor, Y : torch.Tensor, n_iters : int=1000, alpha : float=0.005) -> None:\n",
    "        \"\"\"\n",
    "        Given X and Y, calculate W using GradientDescent, updating W with alpha over n_iters iterations\n",
    "        :param X (torch.Tensor): The independent data of shape (N,F)\n",
    "        :param Y (torch.Tensor): The dependent data (labels) of shape (N,1)\n",
    "        :param n_iters(int): The number of iterations for gradient descent\n",
    "        :param alpha (float): learning rate of gradient descent\n",
    "        :return None\n",
    "        \"\"\"\n",
    "        X = torch.cat([X, torch.ones((X.shape[0],1))], axis=1)\n",
    "        W = torch.zeros((X.shape[1],1))\n",
    "\n",
    "        for i in range(n_iters):\n",
    "            #print(self.gradfn(W, X, Y))\n",
    "            W = W - alpha*self.gradfn(W, X, Y)\n",
    "        \n",
    "        self.W = W[0:-1]\n",
    "        self.b = W[-1]\n",
    "\n",
    "    def gradfn(self, W, X, Y):\n",
    "        \"\"\"\n",
    "        Function that calculates the gradient \n",
    "        :param W (torch.Tensor): tensor of shape (F) containing current guess of weights (and bias)\n",
    "        :param X (torch.Tensor): tensor of shape (N,F) of input features\n",
    "        :param Y (torch.Tensor): target y values\n",
    "        :Return gradient of each weight evaluated at the current value\n",
    "        \"\"\"\n",
    "        return X.transpose(0,1).matmul(X.matmul(W)-Y)/X.shape[0]\n",
    "    \n",
    "    def regress(self, X) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Given X, the tensor of independent data, calculate Y using the weigths trained\n",
    "        :param X (torch.Tensor): The tensor of independent data of shape (N, F)\n",
    "        :return (torch.Tensor): a tensor of shape (N,1) containing the regressed data\n",
    "        \"\"\"\n",
    "        return X.matmul(self.W) + self.b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\n",
      "tensor([[0.7517],\n",
      "        [0.7678]]) tensor([-0.3895])\n",
      "R2 score of our GD regressor is 0.38012903928756714\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test your progress\n",
    "GDReg = GDRegressor()\n",
    "print(GDReg.W, GDReg.b)\n",
    "\n",
    "X = torch.rand((5,2))\n",
    "Y = torch.rand((5,1))\n",
    "GDReg.fit(X, Y, n_iters=10000, alpha=1)\n",
    "print(GDReg.W, GDReg.b)\n",
    "\n",
    "# Regression metrics are kind of arbitrary as the data is generated randomly and some \n",
    "# generation run will produce data that is better-suited for linear regression\n",
    "# while others may produce data that cannot be approximated well by a linear model.\n",
    "Y_pred = GDReg.regress(X)\n",
    "R2torch = R2Score()\n",
    "print(f'R2 score of our GD regressor is {R2torch(Y_pred,Y)}\\n\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score of our GD regressor is 1.0\n",
      "\n",
      "\n",
      "R2 score of our LS regressor is 1.0\n",
      "\n",
      "\n",
      "R2 score of SKlearn regressor is 0.99999999999968\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "X, Y = make_regression(n_samples=10000, n_features=20)\n",
    "# Standardizing data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X).astype(np.float32)\n",
    "\n",
    "X_np = X.copy().astype(np.float32)\n",
    "Y_np = Y.copy().astype(np.float32)\n",
    "\n",
    "X = torch.from_numpy(X.astype(np.float32))\n",
    "Y = torch.from_numpy(Y.astype(np.float32))[:,None]\n",
    "\n",
    "GDReg = GDRegressor()\n",
    "GDReg.fit(X,Y, n_iters=1000, alpha=1)\n",
    "# print(GDReg.W, GDReg.b, '\\n')\n",
    "y_pred_gd = GDReg.regress(X)\n",
    "R2torch = R2Score()\n",
    "print(f'R2 score of our GD regressor is {R2torch(y_pred_gd,Y)}\\n\\n')\n",
    "\n",
    "## Least Square\n",
    "LSReg = LeastSquareRegressor()\n",
    "LSReg.fit(X,Y)\n",
    "# print(LSReg.W, LSReg.b, '\\n')\n",
    "y_pred_torch = LSReg.regress(X)\n",
    "R2torch = R2Score()\n",
    "print(f'R2 score of our LS regressor is {R2torch(y_pred_torch,Y)}\\n\\n')\n",
    "\n",
    "## Compare the results with the sklearn regressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "SKReg = LinearRegression()\n",
    "SKReg.fit(X_np,Y_np)\n",
    "# print(SKReg.coef_, SKReg.intercept_, '\\n')\n",
    "y_pred_sk = X_np.dot(SKReg.coef_) + SKReg.intercept_\n",
    "print(f'R2 score of SKlearn regressor is {r2_score(Y_np, y_pred_sk)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('deepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c1368ead719bab1172f6be1cdedc075cc408ce16edb1ac759f3128a87e8e2e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
